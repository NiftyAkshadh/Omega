#Importing Libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import warnings
warnings.filterwarnings("ignore")

# Load and Inspect the Dataset

file_path = "Google 2004-2024 stock price.csv"  # Ensure the correct path
df = pd.read_csv(file_path, encoding='utf-8-sig')

print("\nFirst 5 rows of the dataset:")
print(df.head())
print("\nLast 5 rows of the dataset:")
print(df.tail())
print("\nDataset Information:")
df.info()
print("\nSummary Statistics:")
print(df.describe())
print("\n The Columns:")
print(df.columns)
print("\n Data types:")
print(df.dtypes)
#Handle Missing Values and Duplicates

print("\nMissing Values Count:")
print(df.isnull().sum())
df.ffill(inplace=True)

print("\nMissing Values Count After Handling:")
print(df.isnull().sum())

df.drop_duplicates(inplace=True)
print("\nDataset Shape After Cleaning (Rows, Columns):", df.shape)


#Convert Date Column to Datetime Format & Extract Features

if 'Date' in df.columns:
    df['Date'] = pd.to_datetime(df['Date'], errors='coerce', utc=True)
    df.dropna(subset=['Date'], inplace=True)
    df.set_index('Date', inplace=True)
    df['Year'] = df.index.year
    df['Month'] = df.index.month
    df['Day'] = df.index.day
    df['Day_of_Week'] = df.index.dayofweek
    df['Is_Weekend'] = df['Day_of_Week'].apply(lambda x: 1 if x >= 5 else 0)
    print("\nDate column converted and new time-based features added.")
else:
    print("\nWarning: 'Date' column not found!")


#Create Moving Averages and Compute Stock Returns

df['MA_7'] = df['Close'].rolling(window=7).mean()
df['MA_30'] = df['Close'].rolling(window=30).mean()
df['Daily_Return'] = df['Close'].pct_change() * 100
df['Volatility_30'] = df['Daily_Return'].rolling(window=30).std()
# Momentum Indicators
df['RSI_14'] = 100 - (100 / (1 + (df['High'].diff(1).clip(lower=0).rolling(14).mean() / 
                                 df['Low'].diff(1).clip(upper=0).abs().rolling(14).mean())))
# MACD
exp12 = df['Close'].ewm(span=12, adjust=False).mean()
exp26 = df['Close'].ewm(span=26, adjust=False).mean()
df['MACD'] = exp12 - exp26
# Bollinger Bands
df['MiddleBand'] = df['Close'].rolling(20).mean()
df['UpperBand'] = df['MiddleBand'] + 1.96*df['Close'].rolling(20).std()
df['LowerBand'] = df['MiddleBand'] - 1.96*df['Close'].rolling(20).std()
df.dropna(inplace=True)  # Drop NaN values generated by rolling functions
print("\nMoving Averages, Daily Returns, and Volatility Metrics added.")


#Prepare Data for Machine Learning

features = ['Open', 'High', 'Low', 'Volume', 'MA_7', 'MA_30', 'Daily_Return', 'Volatility_30']
X = df[features]
y = df['Close']

# Time-Series Cross Validation
from sklearn.model_selection import TimeSeriesSplit
tscv = TimeSeriesSplit(n_splits=5)
for train_index, test_index in tscv.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print("\nData split into training and testing sets.")

features += ['RSI_14', 'MACD', 'MiddleBand', 'UpperBand', 'LowerBand']

#Linear Regression Model

X = df[["Open", "High", "Low", "Volume"]]  
y = df["Close"]

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=537)

from sklearn.linear_model import LinearRegression, Ridge
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
import time
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

models = {
    "Linear Regression": LinearRegression(),
    "Ridge Regression": Ridge(alpha=1.0),
    "Random Forest": RandomForestRegressor(n_estimators=100, random_state=42),
    "Gradient Boosting": GradientBoostingRegressor(n_estimators=100, random_state=42),
    "Support Vector Regression": SVR(kernel='rbf')
}

import time
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
results = []
for name, model in models.items():
    start_time = time.time()
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    end_time = time.time()
    execution_time = end_time - start_time
    results.append({
        "Model": name,
        "Mean Absolute Error": mean_absolute_error(y_test, y_pred),
        "Mean Squared Error": mean_squared_error(y_test, y_pred),
        "R² Score": r2_score(y_test, y_pred),
        "Training Time (s)": execution_time
    })
import pandas as pd
results_df = pd.DataFrame(results)
print(results_df)

# Time Series Analysis

# Seasonal Decomposition using STL
from statsmodels.tsa.seasonal import STL

plt.figure(figsize=(12,8))
stl = STL(df['Close'], period=365)  # Annual seasonality
result = stl.fit()
fig = result.plot()
plt.suptitle('STL Decomposition of Google Stock Prices', y=1.02)
plt.tight_layout()
plt.savefig("stl_decomposition.png")
plt.close()

### Autocorrelation Analysis ###
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

plt.figure(figsize=(12,6))
plot_acf(df['Close'], lags=365, alpha=0.05, title='Autocorrelation (1 Year Lags)')
plt.savefig("acf_plot.png")
plt.close()

plt.figure(figsize=(12,6))
plot_pacf(df['Close'], lags=90, alpha=0.05, title='Partial Autocorrelation (90 Days)')
plt.savefig("pacf_plot.png")
plt.close()

# Prophet Forecasting Integration
from prophet import Prophet  

prophet_df = df.reset_index()[['Date', 'Close']].rename(
    columns={'Date': 'ds', 'Close': 'y'}
)
prophet_df['ds'] = prophet_df['ds'].dt.tz_localize(None)

train_size = int(len(prophet_df) * 0.8)
prophet_train = prophet_df.iloc[:train_size]
prophet_test = prophet_df.iloc[train_size:]

prophet_model = Prophet(
    yearly_seasonality=True,
    weekly_seasonality=True,
    daily_seasonality=False
)
prophet_model.fit(prophet_train)

future = prophet_model.make_future_dataframe(periods=len(prophet_test))
forecast = prophet_model.predict(future)

fig1 = prophet_model.plot_components(forecast)
plt.suptitle('Prophet Forecast Components', y=1.02)
plt.tight_layout()
plt.savefig("prophet_components.png")
plt.close()

from arch import arch_model

returns = df['Daily_Return'].dropna()
am = arch_model(returns * 100, vol='Garch', p=1, q=1)
res = am.fit(update_freq=5)

plt.figure(figsize=(12,6))
res.plot(annualize='D')
plt.suptitle('GARCH(1,1) Volatility Analysis', y=0.95)
plt.tight_layout()
plt.savefig("garch_volatility.png")
plt.close()

print("\nGenerated Plots:")
print(" - stl_decomposition.png")
print(" - acf_plot.png")
print(" - pacf_plot.png")
print(" - prophet_components.png")
print(" - garch_volatility.png")


# Deep Learning Model

from tensorflow.keras.models import Sequential # type: ignore
from tensorflow.keras.layers import LSTM, Dense # type: ignore
from tensorflow.keras.optimizers import Adam # type: ignore
import time
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
df_scaled = scaler.fit_transform(df[["Open", "High", "Low", "Volume", "Close"]])
def create_sequences(data, seq_length):
    X, y = [], []
    for i in range(len(data) - seq_length):
        X.append(data[i:i+seq_length, :-1])  # Features
        y.append(data[i+seq_length, -1])  # Target (Next Close Price)
    return np.array(X), np.array(y)

seq_length = 20
X, y = create_sequences(df_scaled, seq_length)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=537, shuffle=False)

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

model = Sequential([
    LSTM(50, return_sequences=True, input_shape=(seq_length, X.shape[2])),
    LSTM(50, return_sequences=False),
    Dense(25, activation='relu'),
    Dense(1)  # Output layer
])

model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')
 
start_time = time.time()
history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test), verbose=1)
end_time = time.time()
training_time = end_time - start_time

y_pred = model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
results_df = pd.DataFrame({
    "Model": ["LSTM Deep Learning"],
    "Mean Absolute Error": [mae],
    "Mean Squared Error": [mse],
    "R² Score": [r2],
    "Training Time (s)": [training_time]
})
print("LSTM Model Performance Metrics")
print(results_df)

import numpy as np
import matplotlib.pyplot as plt
metrics = ["Mean Absolute Error", "Mean Squared Error", "R² Score", "Training Time (s)"]
values = results_df.iloc[0, 1:].values  # Extract numeric values
values = np.abs(values)  # Convert negative values to positive

fig, axes = plt.subplots(2, 2, figsize=(12, 8))

for i, ax in enumerate(axes.flat):
    total = sum(values)  
    if total == 0:
        continue  # Skip empty values
    
    ax.pie([values[i], total - values[i]], labels=[metrics[i], "Other"], 
           autopct='%1.1f%%', colors=['#ff9999','#66b3ff'], startangle=140)
    ax.set_title(metrics[i])
plt.suptitle("LSTM Model Performance Metrics", fontsize=14)
plt.tight_layout()
plt.show()


#Apply Decision Tree Regression Model**
features = ['Open', 'High', 'Low', 'Volume']
X = df[features]
y = df['Close']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

dt_model = DecisionTreeRegressor(max_depth=5, random_state=42)
dt_model.fit(X_train, y_train)
y_pred = dt_model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)


print(f"Decision Tree Model Performance:")
print(f"Mean Absolute Error (MAE): {mae:.2f}")
print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"R² Score: {r2:.2f}")

plt.figure(figsize=(8, 5))
sns.barplot(x=dt_model.feature_importances_, y=features, palette="viridis")
plt.xlabel("Feature Importance")
plt.ylabel("Features")
plt.title("Feature Importance in Decision Tree Model")
plt.show()

plt.figure(figsize=(10, 5))
plt.scatter(y_test, y_pred, alpha=0.6, color='blue')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle="--")
plt.xlabel("Actual Close Price")
plt.ylabel("Predicted Close Price")
plt.title("Actual vs Predicted Stock Prices (Decision Tree)")
plt.grid()
plt.show()

# Evaluation of Model Performance: Error Metrics, Accuracy, and Training Time

sns.set_style("whitegrid")
fig, axes = plt.subplots(2, 2, figsize=(14, 10))
sns.barplot(x="Model", y="Mean Absolute Error", data=results_df, ax=axes[0, 0], palette="Blues")
axes[0, 0].set_title("Mean Absolute Error (Lower is Better)")
axes[0, 0].set_xticklabels(axes[0, 0].get_xticklabels(), rotation=30, ha="right")
sns.barplot(x="Model", y="Mean Squared Error", data=results_df, ax=axes[0, 1], palette="Greens")
axes[0, 1].set_title("Mean Squared Error (Lower is Better)")
axes[0, 1].set_xticklabels(axes[0, 1].get_xticklabels(), rotation=30, ha="right")
sns.barplot(x="Model", y="R² Score", data=results_df, ax=axes[1, 0], palette="Reds")
axes[1, 0].set_title("R² Score (Higher is Better)")
axes[1, 0].set_xticklabels(axes[1, 0].get_xticklabels(), rotation=30, ha="right")
sns.barplot(x="Model", y="Training Time (s)", data=results_df, ax=axes[1, 1], palette="Purples")
axes[1, 1].set_title("Training Time (Seconds) (Lower is Better)")
axes[1, 1].set_xticklabels(axes[1, 1].get_xticklabels(), rotation=30, ha="right")
plt.tight_layout()
plt.show()

#Classical Time Series Analysis

from statsmodels.tsa.stattools import adfuller

result = adfuller(df['Close'])
print('ADF Statistic:', result[0])
print('p-value:', result[1])

#Data Visualization and Trend Analysis

#Line Plot of Stock Prices
plt.figure(figsize=(12, 6))
plt.plot(df.index, df['Adj Close'], label='Adj Close', color='blue')
plt.plot(df.index, df['Close'], label='Close', color='green', linestyle='dashed')
plt.xlabel('Date')
plt.ylabel('Stock Price')
plt.title('Stock Price Over Time')
plt.legend()
plt.grid()
plt.show()

#Volume Over Time
plt.figure(figsize=(12, 4))
plt.bar(df.index, df['Volume'], color='purple', alpha=0.6)
plt.xlabel('Date')
plt.ylabel('Volume')
plt.title('Trading Volume Over Time')
plt.show()

#High vs. Low Price Range
plt.figure(figsize=(12, 6))
plt.fill_between(df.index, df['High'], df['Low'], color='gray', alpha=0.3, label='High-Low Range')
plt.plot(df.index, df['Close'], color='red', label='Close Price')
plt.xlabel('Date')
plt.ylabel('Price')
plt.title('Stock Price Range Over Time')
plt.legend()
plt.grid()
plt.show()

#Box Plot for Price Distribution
plt.figure(figsize=(8, 6))
sns.boxplot(data=df[['Open', 'High', 'Low', 'Close']], palette='Set2')
plt.title('Stock Price Distribution')
plt.show()


plt.figure(figsize=(12, 6))
plt.plot(df.index, df['Close'], label='Closing Price', color='blue')
plt.plot(df.index, df['MA_30'], label='30-Day Moving Average', color='red')
plt.xlabel('Year')
plt.ylabel('Stock Price')
plt.title('Google Stock Price and 30-Day Moving Average')
plt.legend()
plt.show()

#Distribution of Daily Returns
plt.figure(figsize=(8, 4))
sns.histplot(df['Daily_Return'].dropna(), bins=50, kde=True, color='purple')
plt.title('Distribution of Daily Returns')
plt.xlabel('Daily Return (%)')
plt.ylabel('Frequency')
plt.show()

#Correlation Heatmap

plt.figure(figsize=(10, 6))
numeric_df = df.drop(columns=['Year', 'Month', 'Day', 'Day_of_Week', 'Is_Weekend'])
sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Correlation Matrix of Stock Features')
plt.show()

print("\nVisualization Completed.")

#SHAP Feature Importance

import shap
import xgboost as xgb
from joblib import dump, load
import os

xgb_model = xgb.XGBRegressor(n_estimators=100, random_state=42)
xgb_model.fit(X_train, y_train)

explainer = shap.Explainer(xgb_model, X_train)
shap_values = explainer(X_test)

plt.figure()
shap.summary_plot(shap_values, X_test, plot_type="bar", show=False)
plt.title("SHAP Feature Importance (XGBoost)")
plt.tight_layout()
plt.savefig("feature_importance.png")
plt.close()
print("SHAP feature importance image saved as 'feature_importance.png'")

# Save Model Artifacts
from sklearn.preprocessing import StandardScaler
from joblib import dump 

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_train)
selected_features = X.columns.tolist()
dump(selected_features, "selected_features.pkl")
dump(scaler, "scaler.pkl")
dump(xgb_model, "xgb_model.pkl")
print("Model, features, and scaler saved.")

# Model Outputs (like peca image 3 & 4)
xgb_pred = xgb_model.predict(X_test)
xgb_mae = mean_absolute_error(y_test, xgb_pred)
xgb_mse = mean_squared_error(y_test, xgb_pred)
xgb_r2 = r2_score(y_test, xgb_pred)

print("\n XGBoost Model Performance:")
print(f"  Mean Absolute Error : {xgb_mae:.4f}")
print(f"  Mean Squared Error  : {xgb_mse:.4f}")
print(f"  R² Score            : {xgb_r2:.4f}")

from sklearn.preprocessing import MinMaxScaler
import numpy as np

scaler = MinMaxScaler()
df_scaled = scaler.fit_transform(df[["Open", "High", "Low", "Volume", "Close"]])

seq_length = 20
last_sequence = df_scaled[-seq_length:]
future_predictions = []

input_seq = last_sequence[:, :-1]  # Only features (Open, High, Low, Volume)
input_seq = np.expand_dims(input_seq, axis=0)

for _ in range(30):
    pred = model.predict(input_seq)[0]
    future_predictions.append(pred)
    
    new_entry = input_seq[0][-1].reshape(1, -1)
    new_seq = np.append(input_seq[0][1:], new_entry, axis=0)
    input_seq = np.expand_dims(new_seq, axis=0)

fake_close_prices = scaler.inverse_transform(
    np.concatenate([np.zeros((30, 4)), np.array(future_predictions).reshape(-1, 1)], axis=1)
)[:, -1]

future_dates = pd.date_range(start=df.index[-1] + pd.Timedelta(days=1), periods=30, freq='B')

plt.figure(figsize=(12, 6))
plt.plot(future_dates, fake_close_prices, label="Predicted Close Price", color='orange')
plt.xlabel("Date")
plt.ylabel("Predicted Stock Price")
plt.title("🔮 30-Day Future Stock Price Prediction")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.savefig("future_prediction.png")
plt.close()

print("[✓] Future prediction image saved as 'future_prediction.png'")

#Simulated Profitability
df['Signal'] = 0
df['Signal'] = np.where(df['Close'].diff() > 0, 1, -1)
df.loc[df.index[0], 'Signal'] = 0 


initial_investment = 10000
df['Returns'] = df['Signal'] * df['Daily_Return'] / 100
df['Portfolio Value'] = (1 + df['Returns']).cumprod() * initial_investment

plt.figure(figsize=(12, 6))
plt.plot(df.index, df['Portfolio Value'], label='Simulated Portfolio', color='green')
plt.title(' Simulated Investment Strategy (2004–2024)')
plt.xlabel('Date')
plt.ylabel('Portfolio Value ($)')
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.savefig("profitability_simulation.png")
plt.close()
print("Simulated profitability image saved as 'profitability_simulation.png'")

# Stock Movement Classification (Up/Down) using Random Forest 

# Calculate additional features
df['Price_Change'] = df['Close'].diff()
df['Return_3d'] = df['Close'].pct_change(periods=3)
df['High_Low_Spread'] = df['High'] - df['Low']


from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

df_cls = df.copy()
df_cls['Target_cls'] = (df_cls['Close'].shift(-1) > df_cls['Close']).astype(int)

df_cls.dropna(inplace=True)

features_cls = [
    'Open', 'High', 'Low', 'Volume', 'MA_7', 'MA_30', 'Daily_Return',
    'Volatility_30', 'Price_Change', 'Return_3d', 'High_Low_Spread'
]

X_cls = df_cls[features_cls]
y_cls = df_cls['Target_cls']

X_cls_train, X_cls_test, y_cls_train, y_cls_test = train_test_split(
    X_cls, y_cls, test_size=0.2, random_state=42, shuffle=False
)

# --- Optimized Stock Movement Classification (Up/Down) ---
from xgboost import XGBClassifier
from sklearn.feature_selection import SelectKBest, mutual_info_classif
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import accuracy_score, classification_report, precision_recall_curve
import numpy as np

# Enhanced Feature Engineering
df['RSI_14'] = 100 - (100 / (1 + (df['High'].diff(1).clip(lower=0).rolling(14).mean() / 
                                 df['Low'].diff(1).clip(upper=0).abs().rolling(14).mean())))
exp12 = df['Close'].ewm(span=12, adjust=False).mean()
exp26 = df['Close'].ewm(span=26, adjust=False).mean()
df['MACD'] = exp12 - exp26
df['MiddleBand'] = df['Close'].rolling(20).mean()
df['UpperBand'] = df['MiddleBand'] + 1.96*df['Close'].rolling(20).std()
df['LowerBand'] = df['MiddleBand'] - 1.96*df['Close'].rolling(20).std()

# Prepare Classification Data
df_cls = df.copy()
df_cls['Target_cls'] = (df_cls['Close'].shift(-1) > df_cls['Close']).astype(int)
df_cls.dropna(inplace=True)

# Updated Features List
features_cls = [
    'Open', 'High', 'Low', 'Volume', 'MA_7', 'MA_30', 'Daily_Return',
    'Volatility_30', 'RSI_14', 'MACD', 'MiddleBand', 'UpperBand', 'LowerBand',
    'High_Low_Spread'
]

# Feature Selection
selector = SelectKBest(mutual_info_classif, k=10)
X_cls = selector.fit_transform(df_cls[features_cls], df_cls['Target_cls'])

# Time-Series Cross Validation
tscv = TimeSeriesSplit(n_splits=5)
best_acc = 0
for train_index, test_index in tscv.split(X_cls):
    X_train, X_test = X_cls[train_index], X_cls[test_index]
    y_train, y_test = df_cls['Target_cls'].iloc[train_index], df_cls['Target_cls'].iloc[test_index]
    
    # Dynamic Class Weighting
    scale_pos_weight = len(y_train[y_train==0])/len(y_train[y_train==1])
    
    # Optimized XGBoost
    model = XGBClassifier(
        n_estimators=500,
        max_depth=8,
        learning_rate=0.01,
        subsample=0.8,
        colsample_bytree=0.8,
        scale_pos_weight=scale_pos_weight,
        eval_metric='logloss',
        use_label_encoder=False
    )
    
    model.fit(X_train, y_train)
    y_proba = model.predict_proba(X_test)[:,1]
    
    # Threshold Optimization
    precision, recall, thresholds = precision_recall_curve(y_test, y_proba)
    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)
    best_threshold = thresholds[np.argmax(f1_scores)]
    y_pred = (y_proba >= best_threshold).astype(int)
    
    acc = accuracy_score(y_test, y_pred)
    if acc > best_acc:
        best_acc = acc
        best_model = model
        final_y_test = y_test
        final_y_pred = y_pred

print("\n--- Optimized Classification Results ---")
print(f"Optimal Threshold: {best_threshold:.3f}")
print(f"Accuracy: {best_acc*100:.2f}%")
print(classification_report(final_y_test, final_y_pred, target_names=["Down", "Up"]))

#Summary Output Files
print("\nFiles generated:")
print("  - Feature importance.png")
print("  - selected features.pkl")
print("  - scaler.pkl")
print("  - xgb model.pkl")
print("  - future prediction.png")
print("  - profitability simulation.png")
